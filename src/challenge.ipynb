{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing\n",
    "## testing\n",
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: memory_profiler in /Users/marlonoliveira/opt/anaconda3/lib/python3.9/site-packages (0.61.0)\n",
      "Requirement already satisfied: psutil in /Users/marlonoliveira/opt/anaconda3/lib/python3.9/site-packages (from memory_profiler) (5.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 4.617042303085327 seconds\n",
      "[(datetime.date(2021, 2, 24), 'preetysaini321'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 22), 'preetysaini321'), (datetime.date(2021, 2, 21), 'Surrypuria'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 19), 'Preetm91'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 15), 'jot__b')]\n"
     ]
    }
   ],
   "source": [
    "# %load q1_time.py\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "\n",
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    start_time = time.time()  # Record the start time\n",
    "    \n",
    "    date_tweet_count = defaultdict(int)\n",
    "    date_user_tweets = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "        for line in json_file:\n",
    "            tweet = json.loads(line)\n",
    "            date_str = tweet[\"date\"][:10]  # Extract date part from the datetime string\n",
    "            date_tweet_count[date_str] += 1\n",
    "            date_user_tweets[date_str][tweet[\"user\"][\"username\"]] += 1\n",
    "\n",
    "    top_dates_users = []\n",
    "    for date, user_tweets in date_user_tweets.items():\n",
    "        top_user = max(user_tweets, key=user_tweets.get)\n",
    "        top_dates_users.append((datetime.strptime(date, '%Y-%m-%d').date(), top_user))\n",
    "\n",
    "    end_time = time.time()  # Record the end time\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution Time: {execution_time} seconds\")\n",
    "\n",
    "    return sorted(top_dates_users, key=lambda x: date_tweet_count[x[0]], reverse=True)[:10]\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"/Users/marlonoliveira/Downloads/farmers-protest-tweets-2021-2-4.json\"\n",
    "result = q1_time(file_path)\n",
    "print(result)\n",
    "\n",
    "# Optimizations made:\n",
    "\n",
    "# Instead of using sorted twice, use a single sorted call on top_dates_users with a custom key function to prioritize dates by tweet count.\n",
    "# Use max directly on user_tweets to find the top user, avoiding unnecessary conversion to items.\n",
    "# These optimizations should result in a slightly faster execution time. Keep in mind that the performance gain might vary depending on the size of your dataset and specific conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 4.656842231750488 seconds\n",
      "peak memory: 62.70 MiB, increment: 3.23 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q1_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 4.898242950439453 seconds\n",
      " "
     ]
    }
   ],
   "source": [
    "%prun q1_time(file_path)\n",
    "#%timeit q1_time(file_path)\n",
    "#%time q1_time(file_path)\n",
    "#result = q1_time(file_path)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load q1_memory.py\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "\n",
    "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    date_tweet_count = defaultdict(int)\n",
    "    date_user_tweets = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "        for line in json_file:\n",
    "            tweet = json.loads(line)\n",
    "            date_str = tweet[\"date\"][:10]  # Extract date part from the datetime string\n",
    "            date_tweet_count[date_str] += 1\n",
    "            date_user_tweets[date_str][tweet[\"user\"][\"username\"]] += 1\n",
    "\n",
    "    top_dates_users = []\n",
    "    for date, user_tweets in date_user_tweets.items():\n",
    "        top_user = max(user_tweets, key=user_tweets.get)\n",
    "        top_dates_users.append((datetime.strptime(date, '%Y-%m-%d').date(), top_user))\n",
    "\n",
    "    return sorted(top_dates_users, key=lambda x: date_tweet_count[x[0]], reverse=True)[:10]\n",
    "\n",
    "# Example usage:\n",
    "#file_path = \"/Users/marlonoliveira/Downloads/farmers-protest-tweets-2021-2-4.json\"\n",
    "#result = q1_memory(file_path)\n",
    "#print(result)\n",
    "\n",
    "# Optimizations made to reduce memory usage:\n",
    "\n",
    "# Process the JSON file line by line, minimizing the amount of data loaded into memory at once.\n",
    "# Only store necessary information (date, tweet count, and user tweets) in dictionaries.\n",
    "# These optimizations should help reduce memory usage while still achieving the desired functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.33 s, sys: 380 ms, total: 4.71 s\n",
      "Wall time: 4.73 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 24), 'preetysaini321'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 22), 'preetysaini321'),\n",
       " (datetime.date(2021, 2, 21), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 64.17 MiB, increment: 0.32 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load q2_time.py\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Tuple\n",
    "\n",
    "def extract_emojis(text: str) -> List[str]:\n",
    "    emoji_pattern = r'\\uD83C[\\uDF00-\\uDFFF]|\\uD83D[\\uDC00-\\uDDFF]|\\uD83E[\\uDD00-\\uDDFF]|[\\u2600-\\u2B55]'\n",
    "    emojis = [match.group() for match in re.finditer(emoji_pattern, text)]\n",
    "    return emojis\n",
    "\n",
    "def process_tweet(line):\n",
    "    tweet = json.loads(line)\n",
    "    emojis = extract_emojis(tweet[\"content\"])\n",
    "    return emojis\n",
    "\n",
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    emoji_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            emojis_lists = list(executor.map(process_tweet, json_file))\n",
    "\n",
    "    for emojis in emojis_lists:\n",
    "        emoji_counter.update(emojis)\n",
    "\n",
    "    # Get the top 10 most used emojis\n",
    "    top_emojis = emoji_counter.most_common(10)\n",
    "\n",
    "    return top_emojis\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"/Users/marlonoliveira/Downloads/farmers-protest-tweets-2021-2-4.json\"\n",
    "result = q2_time(file_path)\n",
    "print(result)\n",
    "\n",
    "\n",
    "#This modification uses the ThreadPoolExecutor to concurrently process tweets and extract emojis, which can improve the overall execution time, especially when dealing with a large number of tweets. Keep in mind that the effectiveness of parallelization depends on factors such as the number of available CPU cores and the nature of the processing tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load q2_memory.py\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Tuple\n",
    "\n",
    "def extract_emojis(text: str) -> List[str]:\n",
    "    emoji_pattern = r'\\uD83C[\\uDF00-\\uDFFF]|\\uD83D[\\uDC00-\\uDDFF]|\\uD83E[\\uDD00-\\uDDFF]|[\\u2600-\\u2B55]'\n",
    "    emojis = [match.group() for match in re.finditer(emoji_pattern, text)]\n",
    "    return emojis\n",
    "\n",
    "def process_tweet(line, emoji_counter):\n",
    "    tweet = json.loads(line)\n",
    "    emojis = extract_emojis(tweet[\"content\"])\n",
    "    emoji_counter.update(emojis)\n",
    "\n",
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    emoji_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            executor.map(lambda line: process_tweet(line, emoji_counter), json_file)\n",
    "\n",
    "    # Get the top 10 most used emojis\n",
    "    top_emojis = emoji_counter.most_common(10)\n",
    "\n",
    "    return top_emojis\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"/Users/marlonoliveira/Downloads/farmers-protest-tweets-2021-2-4.json\"\n",
    "result = q2_memory(file_path)\n",
    "print(result)\n",
    "\n",
    "# In this version, we process each line individually, and as soon as we extract the emojis from a tweet, we update the Counter. This way, we avoid keeping a large list of emojis in memory.\n",
    "\n",
    "# This optimization should help reduce memory usage, especially when dealing with a large number of tweets in the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load q3_time.py\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Tuple\n",
    "\n",
    "def extract_mentions(text: str) -> List[str]:\n",
    "    mention_pattern = r'@(\\w+)'  # Adjust the regex pattern as needed\n",
    "    mentions = re.findall(mention_pattern, text)\n",
    "    return mentions\n",
    "\n",
    "def process_tweet(line, mention_counter):\n",
    "    tweet = json.loads(line)\n",
    "    mentions = extract_mentions(tweet[\"content\"])\n",
    "    mention_counter.update(mentions)\n",
    "\n",
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    mention_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            executor.map(lambda line: process_tweet(line, mention_counter), json_file)\n",
    "\n",
    "    # Get the top 10 most mentioned users\n",
    "    top_mentions = mention_counter.most_common(10)\n",
    "\n",
    "    return top_mentions\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"/Users/marlonoliveira/Downloads/farmers-protest-tweets-2021-2-4.json\"\n",
    "result = q3_time(file_path)\n",
    "print(result)\n",
    "\n",
    "# This version uses ThreadPoolExecutor to parallelize the processing of tweets. The process_tweet function extracts mentions from each tweet, and the ThreadPoolExecutor efficiently distributes the workload across multiple threads.\n",
    "\n",
    "# Note: The effectiveness of parallelization depends on the number of available CPU cores and the nature of the processing tasks. Adjust the regex pattern (mention_pattern) based on your specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load q3_memory.py\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Tuple\n",
    "\n",
    "def extract_mentions(text: str) -> List[str]:\n",
    "    mention_pattern = r'@(\\w+)'  # Adjust the regex pattern as needed\n",
    "    mentions = re.findall(mention_pattern, text)\n",
    "    return mentions\n",
    "\n",
    "def process_tweet(line, mention_counter):\n",
    "    tweet = json.loads(line)\n",
    "    mentions = extract_mentions(tweet[\"content\"])\n",
    "    mention_counter.update(mentions)\n",
    "\n",
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    mention_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            executor.map(lambda line: process_tweet(line, mention_counter), json_file)\n",
    "\n",
    "    # Get the top 10 most mentioned users\n",
    "    top_mentions = mention_counter.most_common(10)\n",
    "\n",
    "    return top_mentions\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"/Users/marlonoliveira/Downloads/farmers-protest-tweets-2021-2-4.json\"\n",
    "result = q3_memory(file_path)\n",
    "print(result)\n",
    "\n",
    "# To optimize the code for memory usage, we can make some modifications to reduce the memory footprint. Specifically, we can avoid storing the entire list of mentions in memory and update the counter directly as we process each line\n",
    "\n",
    "# In this version, the mention_counter is updated directly without storing the entire list of mentions in memory. This optimization helps reduce the memory footprint, especially when dealing with a large number of tweets in the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST request was successful!\n",
      "Response: {\"status\":\"OK\",\"detail\":\"your request was received\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the URL and JSON payload\n",
    "url = \"https://advana-challenge-check-api-cr-k4hdbggvoq-uc.a.run.app/data-engineer\"\n",
    "payload = {\n",
    "    \"name\": \"Marlon Oliveira\",\n",
    "    \"mail\": \"oliwer.marlon@gmail.com\",\n",
    "    \"github_url\": \"https://github.com/marlondcu/latam-challenge.git\"\n",
    "}\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    print(\"POST request was successful!\")\n",
    "    print(\"Response:\", response.text)\n",
    "else:\n",
    "    print(\"POST request failed with status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
