{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latam Challenge!\n",
    "\n",
    "First step is to set the File Path, in this example the file is in the same foldes as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing need packages to measure memory usage, the following line is commented, assuming it is already installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the package memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's begin to import the challenge files, we are using load to load an external file. \n",
    "\n",
    "First challenge:\n",
    "The top 10 dates with the most tweets. Mention the user (username) with the most posts for each of those days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load q1_time.py\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "\n",
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    date_tweet_count = defaultdict(int)\n",
    "    date_user_tweets = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file: # import json content\n",
    "        for line in json_file:\n",
    "            tweet = json.loads(line)\n",
    "            date_str = tweet[\"date\"][:10]  # extract date part from the datetime string\n",
    "            date_tweet_count[date_str] += 1 # count tweets\n",
    "            date_user_tweets[date_str][tweet[\"user\"][\"username\"]] += 1\n",
    "\n",
    "    top_dates_users = []\n",
    "    for date, user_tweets in date_user_tweets.items():\n",
    "        top_user = max(user_tweets, key=user_tweets.get)\n",
    "        top_dates_users.append((datetime.strptime(date, '%Y-%m-%d').date(), top_user))\n",
    "\n",
    "    return sorted(top_dates_users, key=lambda x: date_tweet_count[x[0]], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code runs the function, returns the expected and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "file_path = \"/Users/marlonoliveira/Downloads/farmers-protest-tweets-2021-2-4.json\"\n",
    "result = q1_time(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Optimizations:\n",
    "\n",
    "* Used a single sorted call on top_dates_users with a custom key function to prioritize dates by tweet count.\n",
    "* Used max directly on user_tweets to find the top user, avoiding unnecessary conversion to items.\n",
    "\n",
    "Note the code bellow, using %prun command, we can see the execution time and the details about it. Another commands are commented because they do similar tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun q1_time(file_path)\n",
    "#%timeit q1_time(file_path)\n",
    "#%time q1_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code bellow measures the memory usage during the execution of the q1_time function, note it is only to show that after optimization it reduces memora usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit q1_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said before, the following code contains memory usage optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load q1_memory.py\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "\n",
    "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    date_tweet_count = defaultdict(int)\n",
    "    date_user_tweets = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "        for line in json_file:\n",
    "            tweet = json.loads(line)\n",
    "            date_str = tweet[\"date\"][:10]  # Extract date part from the datetime string\n",
    "            date_tweet_count[date_str] += 1\n",
    "            date_user_tweets[date_str][tweet[\"user\"][\"username\"]] += 1\n",
    "\n",
    "    top_dates_users = []\n",
    "    for date, user_tweets in date_user_tweets.items():\n",
    "        top_user = max(user_tweets, key=user_tweets.get)\n",
    "        top_dates_users.append((datetime.strptime(date, '%Y-%m-%d').date(), top_user))\n",
    "\n",
    "    return sorted(top_dates_users, key=lambda x: date_tweet_count[x[0]], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizations made to reduce memory usage:\n",
    "\n",
    "* Process the JSON file line by line, minimizing the amount of data loaded into memory at once.\n",
    "* Only store necessary information (date, tweet count, and user tweets) in dictionaries.\n",
    "* These optimizations help reduce memory usage while still achieving the desired functionality.\n",
    "\n",
    "The code bellow runs the funcion, save the return in a variable called 'result' and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/marlonoliveira/Downloads/farmers-protest-tweets-2021-2-4.json\"\n",
    "result = q1_memory(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code bellow measures the execution time, however we are not interested in time optimization at this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The following code show the memory usage. Now we are interested in seeing how the optimization improves the memory usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second challenge:\n",
    "The top 10 most used emojis with their respective counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load q2_time.py\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Tuple\n",
    "\n",
    "def extract_emojis(text: str) -> List[str]:\n",
    "    emoji_pattern = r'\\uD83C[\\uDF00-\\uDFFF]|\\uD83D[\\uDC00-\\uDDFF]|\\uD83E[\\uDD00-\\uDDFF]|[\\u2600-\\u2B55]'\n",
    "    emojis = [match.group() for match in re.finditer(emoji_pattern, text)]\n",
    "    return emojis\n",
    "\n",
    "def process_tweet(line):\n",
    "    tweet = json.loads(line)\n",
    "    emojis = extract_emojis(tweet[\"content\"])\n",
    "    return emojis\n",
    "\n",
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    emoji_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            emojis_lists = list(executor.map(process_tweet, json_file))\n",
    "\n",
    "    for emojis in emojis_lists:\n",
    "        emoji_counter.update(emojis)\n",
    "\n",
    "    # Get the top 10 most used emojis\n",
    "    top_emojis = emoji_counter.most_common(10)\n",
    "\n",
    "    return top_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This modification uses the ThreadPoolExecutor to concurrently process tweets and extract emojis, which can improve the overall execution time, especially when dealing with a large number of tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "file_path = \"/Users/marlonoliveira/Downloads/farmers-protest-tweets-2021-2-4.json\"\n",
    "result = q2_time(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code show the execution times for the funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun q1_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code bellow show the same code, however now optimized for memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load q2_memory.py\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Tuple\n",
    "\n",
    "def extract_emojis(text: str) -> List[str]:\n",
    "    emoji_pattern = r'\\uD83C[\\uDF00-\\uDFFF]|\\uD83D[\\uDC00-\\uDDFF]|\\uD83E[\\uDD00-\\uDDFF]|[\\u2600-\\u2B55]'\n",
    "    emojis = [match.group() for match in re.finditer(emoji_pattern, text)]\n",
    "    return emojis\n",
    "\n",
    "def process_tweet(line, emoji_counter):\n",
    "    tweet = json.loads(line)\n",
    "    emojis = extract_emojis(tweet[\"content\"])\n",
    "    emoji_counter.update(emojis)\n",
    "\n",
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    emoji_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            executor.map(lambda line: process_tweet(line, emoji_counter), json_file)\n",
    "\n",
    "    top_emojis = emoji_counter.most_common(10) # get the top 10 most used emojis\n",
    "\n",
    "    return top_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version, we process each line individually, and as soon as we extract the emojis from a tweet, we update the Counter. This way, we avoid keeping a large list of emojis in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code show the memory usage. Now we are interested in seeing how the optimization improves the memory usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third challenge:\n",
    "The historical top 10 users (username) most influential based on the count of mentions (@) they register. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load q3_time.py\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Tuple\n",
    "\n",
    "def extract_mentions(text: str) -> List[str]:\n",
    "    mention_pattern = r'@(\\w+)'  # Adjust the regex pattern as needed\n",
    "    mentions = re.findall(mention_pattern, text)\n",
    "    return mentions\n",
    "\n",
    "def process_tweet(line, mention_counter):\n",
    "    tweet = json.loads(line)\n",
    "    mentions = extract_mentions(tweet[\"content\"])\n",
    "    mention_counter.update(mentions)\n",
    "\n",
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    mention_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            executor.map(lambda line: process_tweet(line, mention_counter), json_file)\n",
    "\n",
    "    # Get the top 10 most mentioned users\n",
    "    top_mentions = mention_counter.most_common(10)\n",
    "\n",
    "    return top_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version uses ThreadPoolExecutor to parallelize the processing of tweets. The process_tweet function extracts mentions from each tweet, and the ThreadPoolExecutor efficiently distributes the workload across multiple threads.\n",
    "\n",
    "# Note: The effectiveness of parallelization depends on the number of available CPU cores and the nature of the processing tasks. Adjust the regex pattern (mention_pattern) based on your specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "result = q3_time(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code show the execution times for the funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun q3_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code is to optimize the memory usage of the second challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load q3_memory.py\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Tuple\n",
    "\n",
    "def extract_mentions(text: str) -> List[str]:\n",
    "    mention_pattern = r'@(\\w+)'  # Adjust the regex pattern as needed\n",
    "    mentions = re.findall(mention_pattern, text)\n",
    "    return mentions\n",
    "\n",
    "def process_tweet(line, mention_counter):\n",
    "    tweet = json.loads(line)\n",
    "    mentions = extract_mentions(tweet[\"content\"])\n",
    "    mention_counter.update(mentions)\n",
    "\n",
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    mention_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            executor.map(lambda line: process_tweet(line, mention_counter), json_file)\n",
    "\n",
    "    # Get the top 10 most mentioned users\n",
    "    top_mentions = mention_counter.most_common(10)\n",
    "\n",
    "    return top_mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For memory usage optimization, I make some modifications to reduce the memory footprint. Specifically, avoiding storing the entire list of mentions in memory and update the counter directly as we process each line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code show the memory usage. Now we are interested in seeing how the optimization improves the memory usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folling code makes a Post request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define the URL and JSON \n",
    "url = \"https://advana-challenge-check-api-cr-k4hdbggvoq-uc.a.run.app/data-engineer\"\n",
    "payload = {\n",
    "    \"name\": \"Marlon Oliveira\",\n",
    "    \"mail\": \"oliwer.marlon@gmail.com\",\n",
    "    \"github_url\": \"https://github.com/marlondcu/latam-challenge.git\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)# Make the POST request\n",
    "\n",
    "if response.status_code == 200: # Check if the request was successful (status code 200)\n",
    "    print(\"POST request was successful!\")\n",
    "    print(\"Response:\", response.text)\n",
    "else:\n",
    "    print(\"POST request failed with status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
