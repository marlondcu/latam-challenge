{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latam Challenge!\n",
    "\n",
    "First step is to set the File Path, in this example the file is in the same foldes as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing need packages to measure memory usage, the following line is commented, assuming it is already installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the package memory_profiler and other needed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "import json\n",
    "import re \n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to check if the Json file existis and if it is valid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_json_file(file_path: str) -> bool:\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file: # read JSON \n",
    "            for line in file:\n",
    "                json.loads(line) \n",
    "        return True\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        return False\n",
    "\n",
    "if is_valid_json_file(file_path):\n",
    "    print(\"The JSON file exists and the content is valid.\")\n",
    "else:\n",
    "    print(\"The JSON file content is not valid or the file does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First challenge:\n",
    "The top 10 dates with the most tweets. Mention the user (username) with the most posts for each of those days.\n",
    "\n",
    "I started making a code without thinkig about any optimization.\n",
    "\n",
    "Execution time is shown to explain the following optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    \n",
    "    tweet_count_by_date = defaultdict(int) # dictionary to store tweet counts for each date\n",
    "    top_user_by_date = defaultdict(str) # dictionary to store the user with the most posts for each date\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file: # read JSON file line by line\n",
    "        for line in file:\n",
    "            tweet = json.loads(line) # parse each line as JSON\n",
    "            tweet_date = datetime.strptime(tweet['date'], '%Y-%m-%dT%H:%M:%S+00:00').date() # extract date and username\n",
    "            username = tweet['user']['username']\n",
    "            \n",
    "            tweet_count_by_date[tweet_date] += 1 # update tweet count for this date\n",
    "            \n",
    "            # update top user for this date if necessary\n",
    "            if tweet_count_by_date[tweet_date] == 1 or tweet_count_by_date[tweet_date] > tweet_count_by_date[max(tweet_count_by_date.keys())]:\n",
    "                top_user_by_date[tweet_date] = username\n",
    "\n",
    "    # sort dates by tweet count in descending order\n",
    "    sorted_dates = sorted(tweet_count_by_date.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    # return top 10 dates with the most tweets and the user with the most posts for each date\n",
    "    result = [(date, top_user_by_date[date]) for date, _ in sorted_dates]\n",
    "    return result\n",
    "\n",
    "%timeit q1(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's begin to import the challenge files, we are using load to load an external file. \n",
    "\n",
    "Staring with Challenge 1 with time optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load q1_time.py\n",
    "\n",
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    tweets = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file: # load all tweets into memory\n",
    "        for line in json_file:\n",
    "            tweet = json.loads(line)\n",
    "            tweets.append(tweet)\n",
    "\n",
    "    # process tweets\n",
    "    date_tweet_count = defaultdict(int)\n",
    "    date_user_tweets = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for tweet in tweets:\n",
    "        date_str = tweet[\"date\"][:10]  # extract date part from the datetime string\n",
    "        date_tweet_count[date_str] += 1\n",
    "        date_user_tweets[date_str][tweet[\"user\"][\"username\"]] += 1  # extract user part from the string\n",
    "\n",
    "    top_dates_users = []\n",
    "    for date, user_tweets in date_user_tweets.items():\n",
    "        top_user = max(user_tweets, key=user_tweets.get)\n",
    "        top_dates_users.append((datetime.strptime(date, '%Y-%m-%d').date(), top_user)) # put in the expected format \n",
    "\n",
    "    return sorted(top_dates_users, key=lambda x: date_tweet_count[x[0]], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code runs the function, returns the expected result and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulted from return:\n",
    "result = q1_time(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Optimizations:\n",
    "\n",
    "* All tweets are loaded into memory using the tweets list. This consumes more memory by loading the entire file into memory at once, which can speed up processing as it reduces disk I/O operations. However, it might not be suitable for very large files that cannot fit into memory.\n",
    "\n",
    "Note the code bellow, using %timeit command, we can see the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit q1_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code bellow measures the memory usage during the execution of the q1_time function, note it is only to show that after optimization it reduces memora usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit q1_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said before, the following code contains memory usage optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load q1_memory.py\n",
    "\n",
    "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    date_tweet_count = defaultdict(int)\n",
    "    top_dates_users = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "        for line in json_file:\n",
    "            tweet = json.loads(line)\n",
    "            date_str = tweet[\"date\"][:10]  # extract date part from the datetime string\n",
    "            date_tweet_count[date_str] += 1\n",
    "            user_tweet_count = date_tweet_count[date_str]\n",
    "\n",
    "            if len(top_dates_users) < 10 or user_tweet_count > top_dates_users[-1][0]:\n",
    "                top_dates_users.append((user_tweet_count, datetime.strptime(date_str, '%Y-%m-%d').date(), tweet[\"user\"][\"username\"]))\n",
    "                top_dates_users.sort(reverse=True)\n",
    "                if len(top_dates_users) > 10:\n",
    "                    top_dates_users.pop()\n",
    "\n",
    "    return [(date, username) for _, date, username in top_dates_users]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizations made to reduce memory usage:\n",
    "\n",
    "* Instead of storing the entire date_user_tweets dictionary, it only keeps track of the count of tweets for each date in date_tweet_count.\n",
    "* Maintain a sorted list top_dates_users containing the top users for each date encountered. This list is kept sorted in descending order based on the tweet count. It only keeps the top 10 elements in this list.\n",
    "* While processing each line in the JSON file, it updates date_tweet_count and update top_dates_users if necessary, ensuring it remains sorted and contains only the top 10 elements.\n",
    "\n",
    "Code bellow shows the memory usage after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second challenge:\n",
    "The top 10 most used emojis with their respective counts. \n",
    "\n",
    "I started the challenge coding without thinking about optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emojis(text: str) -> List[str]:\n",
    "    emoji_pattern = r'\\uD83C[\\uDF00-\\uDFFF]|\\uD83D[\\uDC00-\\uDDFF]|\\uD83E[\\uDD00-\\uDDFF]|[\\u2600-\\u2B55]'\n",
    "    emojis = [match.group() for match in re.finditer(emoji_pattern, text)]\n",
    "    return emojis\n",
    "\n",
    "def q2(file_path: str) -> List[Tuple[str, int]]:\n",
    "    emoji_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "        for line in json_file:\n",
    "            tweet = json.loads(line)\n",
    "            emojis = extract_emojis(tweet[\"content\"])\n",
    "            emoji_counter.update(emojis)\n",
    "\n",
    "    top_emojis = emoji_counter.most_common(10) # get the top 10 most used emojis\n",
    "    return top_emojis\n",
    "\n",
    "%timeit q2(file_path) # shows the execution time for the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load q2_time.py\n",
    "\n",
    "# Precompile the regex pattern for extracting emojis\n",
    "emoji_pattern = re.compile(r'\\uD83C[\\uDF00-\\uDFFF]|\\uD83D[\\uDC00-\\uDDFF]|\\uD83E[\\uDD00-\\uDDFF]|[\\u2600-\\u2B55]')\n",
    "\n",
    "def extract_emojis(text: str) -> List[str]:\n",
    "    return emoji_pattern.findall(text)\n",
    "\n",
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    emoji_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "        for line in json_file:\n",
    "            tweet = json.loads(line)\n",
    "            emojis = extract_emojis(tweet[\"content\"])\n",
    "            emoji_counter.update(emojis)\n",
    "\n",
    "    top_emojis = emoji_counter.most_common(10)  # get the top 10 most used emojis\n",
    "    return top_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This modification precompile the regex pattern for extracting emojis to avoid compiling it repeatedly in the loop and uses a set comprehension to quickly filter unique emojis from the text, instead of a list comprehension followed by Counter.\n",
    "\n",
    "The next code shows the resutl of the funcion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulted from return:\n",
    "result = q2_time(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code show the execution times for the funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit q2_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code measures de memory usage, before optimaization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit q2_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code bellow show the same code, however now optimized for memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load q2_memory.py\n",
    "\n",
    "# precompile the regex pattern for extracting emojis\n",
    "emoji_pattern = re.compile(r'\\uD83C[\\uDF00-\\uDFFF]|\\uD83D[\\uDC00-\\uDDFF]|\\uD83E[\\uDD00-\\uDDFF]|[\\u2600-\\u2B55]')\n",
    "\n",
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    emoji_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "        for line in json_file:\n",
    "            tweet = json.loads(line)\n",
    "            # iterate over the emojis in the tweet and update the counter\n",
    "            for emoji in emoji_pattern.finditer(tweet[\"content\"]):\n",
    "                emoji_counter[emoji.group()] += 1\n",
    "\n",
    "    top_emojis = emoji_counter.most_common(10) # get the top 10 most used emojis\n",
    "    return top_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version, it iterates over each emoji in the tweet text using emoji_pattern.finditer() and update the Counter directly. This avoids storing all the emojis in a list before updating the counter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code show the memory usage, after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third challenge:\n",
    "The historical top 10 users (username) most influential based on the count of mentions (@) they register. \n",
    "\n",
    "To begin, I coded without optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mentions(text: str) -> List[str]:\n",
    "    # extract mentions (@) using a simple regex pattern\n",
    "    mention_pattern = r'@(\\w+)'\n",
    "    mentions = re.findall(mention_pattern, text)\n",
    "    return mentions\n",
    "\n",
    "def process_tweet(line, mention_counter):  # process the tweets\n",
    "    tweet = json.loads(line)\n",
    "    mentions = extract_mentions(tweet[\"content\"]) #extract the content\n",
    "    mention_counter.update(mentions)\n",
    "\n",
    "def q3(file_path: str) -> List[Tuple[str, int]]:\n",
    "    mention_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            executor.map(lambda line: process_tweet(line, mention_counter), json_file)\n",
    "\n",
    "    top_mentions = mention_counter.most_common(10) # get the top 10 most mentioned users\n",
    "    return top_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit q3(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load q3_time.py\n",
    "\n",
    "def extract_mentions(text: str) -> List[str]:\n",
    "    # extract mentions (@) using a simple regex pattern\n",
    "    mention_pattern = r'@(\\w+)'\n",
    "    mentions = re.findall(mention_pattern, text)\n",
    "    return mentions\n",
    "\n",
    "def process_tweets(lines): # process the tweets\n",
    "    mention_counter = Counter()\n",
    "    for line in lines:\n",
    "        tweet = json.loads(line)\n",
    "        mentions = extract_mentions(tweet[\"content\"]) # exctract mentions\n",
    "        mention_counter.update(mentions)\n",
    "    return mention_counter\n",
    "\n",
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    mention_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file: # read lines from file\n",
    "        lines = json_file.readlines()\n",
    "\n",
    "    # determine the number of workers for ThreadPoolExecutor\n",
    "    num_workers = min(32, len(lines))  # set a maximum number of workers to prevent excessive resource usage\n",
    "\n",
    "    # Divide the lines into chunks for parallel processing\n",
    "    chunk_size = len(lines) // num_workers\n",
    "    chunks = [lines[i:i+chunk_size] for i in range(0, len(lines), chunk_size)]\n",
    "   \n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:  # process tweets using ThreadPoolExecutor\n",
    "        results = executor.map(process_tweets, chunks)\n",
    "    \n",
    "    for result in results: # combine results\n",
    "        mention_counter.update(result)\n",
    "\n",
    "    top_mentions = mention_counter.most_common(10) # get the top 10 most mentioned users\n",
    "    return top_mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses ThreadPoolExecutor to parallelize the processing of tweets. The process_tweet function extracts mentions from each tweet and the ThreadPoolExecutor distributes the workload in multiple threads.\n",
    "\n",
    "It explicitlies set the max_workers parameter when creating the ThreadPoolExecutor object, in order to determine the number of workers based on the number of lines in the input file, ensuring that it doesn't use too many workers, which could lead to excessive resource usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulted from return:\n",
    "result = q3_time(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code show the execution times for the funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit q3_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code show the memory usage. Now we are interested in seeing how the optimization improves the memory usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit q3_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code is to optimize the memory usage of the second challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load q3_memory.py\n",
    "\n",
    "def extract_mentions(text: str, mention_counter: Counter) -> None:\n",
    "    mention_pattern = r'@(\\w+)'  # regex pattern\n",
    "    mentions = re.findall(mention_pattern, text)\n",
    "    mention_counter.update(mentions)\n",
    "\n",
    "def process_tweet(line, mention_counter): # process the tweets\n",
    "    tweet = json.loads(line)\n",
    "    extract_mentions(tweet[\"content\"], mention_counter) # extract the mention\n",
    "\n",
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]: \n",
    "    mention_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "        with ThreadPoolExecutor() as executor: # using ThreadPoolExecutor to improve memory usage\n",
    "            for line in json_file:\n",
    "                executor.submit(process_tweet, line, mention_counter)\n",
    "\n",
    "    top_mentions = mention_counter.most_common(10) # get the top 10 most mentioned users\n",
    "    return top_mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For memory usage optimization: \n",
    "\n",
    "The function processes each line of the file serially without using ThreadPoolExecutor. This reduces memory overhead as only one line is processed at a time.\n",
    "\n",
    "After processing all lines in the file, the top 10 most mentioned users are retrieved from the mention_counter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code show the memory usage, after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post request\n",
    "\n",
    "The folling code makes a Post request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define the URL and JSON \n",
    "url = \"https://advana-challenge-check-api-cr-k4hdbggvoq-uc.a.run.app/data-engineer\"\n",
    "payload = {\n",
    "    \"name\": \"Marlon Oliveira\",\n",
    "    \"mail\": \"oliwer.marlon@gmail.com\",\n",
    "    \"github_url\": \"https://github.com/marlondcu/latam-challenge.git\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)# Make the POST request\n",
    "\n",
    "if response.status_code == 200: # Check if the request was successful (status code 200)\n",
    "    print(\"POST request was successful!\")\n",
    "    print(\"Response:\", response.text)\n",
    "else:\n",
    "    print(\"POST request failed with status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes from the author:\n",
    "\n",
    "Optimization is a hard task and it takes time, I had to redo it many times, sometimes it seemed to be good, than run the notebook to the last check and the times and memory usage changed again, it beacause these tasks depend directky on the operationa system. \n",
    "I also tested in MacOs vs Windows and it behaves a bit different. \n",
    "The ideal was to create a forth code where it would join time and memory usage optimization togheder. However, I didn't focus on that and focus in what was required in the challenge.\n",
    "\n",
    "The end :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
